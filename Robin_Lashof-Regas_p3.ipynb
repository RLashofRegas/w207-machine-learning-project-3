{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "firstname_lastname_p3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4r1L69aGoZG0"
      },
      "source": [
        "# Project 3: Poisonous Mushrooms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "btVARsAroZG1"
      },
      "source": [
        "In this project, you'll investigate properties of mushrooms. This classic dataset contains over 8000 observations, where each mushroom is described by a variety of features like color, odor, etc., and the target variable is an indicator for whether the mushroom is poisonous. Since all the observations are categorical, I've binarized the feature space. Look at the feature_names below to see all 126 binary names.\n",
        "\n",
        "You'll start by running PCA to reduce the dimensionality from 126 down to 2 so that you can easily visualize the data. In general, PCA is very useful for visualization (though sklearn.manifold.tsne is known to produce better visualizations). Recall that PCA is a linear transformation. The 1st projected dimension is the linear combination of all 126 original features that captures as much of the variance in the data as possible. The 2nd projected dimension is the linear combination of all 126 original features that captures as much of the remaining variance as possible. The idea of dense low dimensional representations is crucial to machine learning!\n",
        "\n",
        "Once you've projected the data to 2 dimensions, you'll experiment with clustering using KMeans and density estimation with Gaussian Mixture Models. Finally, you'll train a classifier by fitting a GMM for the positive class and a GMM for the negative class, and perform inference by comparing the probabilities output by each model.\n",
        "\n",
        "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDvuh15loZG2",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import urllib.request as urllib2 # For python3\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "# additional imports\n",
        "from math import sqrt, ceil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JKyn8WO1pJwK",
        "colab": {}
      },
      "source": [
        "MUSHROOM_DATA = 'https://raw.githubusercontent.com/UCB-MIDS/207-Applied-Machine-Learning/master/Data/mushroom.data'\n",
        "MUSHROOM_MAP = 'https://raw.githubusercontent.com/UCB-MIDS/207-Applied-Machine-Learning/master/Data/mushroom.map'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "28FIvBtVoZG4"
      },
      "source": [
        "Load feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zXUv_S1YoZG4",
        "colab": {}
      },
      "source": [
        "feature_names = []\n",
        "\n",
        "for line in urllib2.urlopen(MUSHROOM_MAP):\n",
        "    [index, name, junk] = line.decode('utf-8').split()\n",
        "    feature_names.append(name)\n",
        "\n",
        "print('Loaded feature names: ', len(feature_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k034gVFboZHA"
      },
      "source": [
        "Load data. The data is sparse in the input file, but there aren't too many features, so we'll use a dense representation, which is supported by all sklearn objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gg6IVy0loZHA",
        "colab": {}
      },
      "source": [
        "X, Y = [], []\n",
        "\n",
        "for line in urllib2.urlopen(MUSHROOM_DATA):\n",
        "    items = line.decode('utf-8').split()\n",
        "    Y.append(int(items.pop(0)))\n",
        "    x = np.zeros(len(feature_names))\n",
        "    for item in items:\n",
        "        feature = int(str(item).split(':')[0])\n",
        "        x[feature] = 1\n",
        "    X.append(x)\n",
        "\n",
        "# Convert these lists to numpy arrays.\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# Split into train and test data.\n",
        "train_data, train_labels = X[:7000], Y[:7000]\n",
        "test_data, test_labels = X[7000:], Y[7000:]\n",
        "\n",
        "# Check that the shapes look right.\n",
        "print(train_data.shape, test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nr25XV7BoZHC"
      },
      "source": [
        "### Part 1:\n",
        "\n",
        "Run Principal Components Analysis on the data. Show what fraction of the total variance in the training data is explained by the first k principal components, for k in [1, 50]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "frHUWcZUoZHC",
        "colab": {}
      },
      "source": [
        "def P1():\n",
        "  pca = PCA(n_components=50)\n",
        "  pca.fit(train_data)\n",
        "  total_explained_variance = 0.0\n",
        "  # loop through array of %variance explained for each component\n",
        "  for k, component in enumerate(pca.explained_variance_ratio_):\n",
        "    # add the variance of the next component\n",
        "    total_explained_variance += component\n",
        "    # print the sum of the first k components\n",
        "    print(\"The first \" + str(k + 1) + \" Pincipal Component(s) Explain \" \n",
        "          + \"{:.2%}\".format(total_explained_variance) + \" of the variance.\")\n",
        "\n",
        "P1()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zbGl1D3RoZHE"
      },
      "source": [
        "### Part 2:\n",
        "\n",
        "PCA can be very useful for visualizing data. Project the training data down to 2 dimensions and plot it. Show the positive (poisonous) cases in blue and the negative (non-poisonous) in red. Here's a reference for plotting: http://matplotlib.org/users/pyplot_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZqPswXVloZHE",
        "colab": {}
      },
      "source": [
        "def P2():\n",
        "  # fit pca and apply dimensionality reduction on X\n",
        "  pca = PCA(n_components=2)\n",
        "  reduced_train_data = pca.fit_transform(train_data)\n",
        "  \n",
        "  # initialize plt axis object\n",
        "  fig_size = 10\n",
        "  fig = plt.figure(figsize=(fig_size, fig_size))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  \n",
        "  # convert train labels to bool for use in array indexing\n",
        "  label_bools = np.array(train_labels, dtype=bool)\n",
        "\n",
        "  # plot separate series using label_bools to allow for legend labels\n",
        "  ax.plot(reduced_train_data[label_bools, 0], \n",
        "          reduced_train_data[label_bools, 1], 'b+', markersize=5)\n",
        "  ax.plot(reduced_train_data[~label_bools, 0], \n",
        "          reduced_train_data[~label_bools, 1], 'r+', markersize=5)\n",
        "  \n",
        "  # set legend, labels and title\n",
        "  ax.legend([\"poisonous\", \"non-poisonous\"])\n",
        "  ax.set_xlabel(\"PC1\")\n",
        "  ax.set_ylabel(\"PC2\")\n",
        "  ax.set_title(\"PC1 vs PC2\")\n",
        "\n",
        "  # show plot\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "P2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yuHTFCyNoZHG"
      },
      "source": [
        "### Part 3:\n",
        "\n",
        "Run KMeans with [1,16] clusters over the 2d projected data. Mark each centroid cluster and plot a circle that goes through the most distant point assigned to each cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJcVJnkPoZHG",
        "colab": {}
      },
      "source": [
        "def P3():\n",
        "  max_clusters = 16\n",
        "\n",
        "  # fit pca and apply dimensionality reduction on X\n",
        "  pca = PCA(n_components=2)\n",
        "  reduced_train_data = pca.fit_transform(train_data)\n",
        "\n",
        "  # initialize plt figure object with correct size given max_clusters\n",
        "  plot_size = 5\n",
        "  num_plots_horiz = 4\n",
        "  num_plots_vert = ceil(max_clusters / 4.0)\n",
        "  fig = plt.figure(figsize=(plot_size * num_plots_horiz, \n",
        "                            plot_size * num_plots_vert))\n",
        "\n",
        "  # loop through k values and add subplots for each\n",
        "  for k in range(1, max_clusters + 1):\n",
        "    ax = fig.add_subplot(num_plots_vert, num_plots_horiz, k)\n",
        "\n",
        "    # fit kmeans on reduced X\n",
        "    kmeans = KMeans(n_clusters = k)\n",
        "    kmeans.fit(reduced_train_data)\n",
        "    \n",
        "    # get centers and max distances to centers\n",
        "    centers = kmeans.cluster_centers_\n",
        "    max_center_distances = np.zeros(len(centers))\n",
        "    for i, label in enumerate(kmeans.labels_):\n",
        "      dist = sqrt((reduced_train_data[i, 0] - centers[label, 0])**2 + \n",
        "                  ((reduced_train_data[i, 1] - centers[label, 1])**2))\n",
        "      if(dist > max_center_distances[label]):\n",
        "        max_center_distances[label] = dist\n",
        "    \n",
        "    # convert train labels to bool for use in array indexing\n",
        "    label_bools = np.array(train_labels, dtype=bool)\n",
        "\n",
        "    # plot separate series using label_bools to allow for legend labels\n",
        "    ax.plot(reduced_train_data[label_bools, 0], \n",
        "            reduced_train_data[label_bools, 1], \n",
        "            'b+', label='poisonous', markersize=5)\n",
        "    ax.plot(reduced_train_data[~label_bools, 0], \n",
        "            reduced_train_data[~label_bools, 1], \n",
        "            'r+', label='non-poisonous', markersize=5)\n",
        "    \n",
        "    # plot cluster centers and circles\n",
        "    ax.plot(centers[:, 0], centers[:, 1], 'gX', markersize=10, label='clusters')\n",
        "    for center, dist in zip(centers, max_center_distances):\n",
        "      circle = plt.Circle(center, dist, color='g', fill=False)\n",
        "      ax.add_artist(circle)\n",
        "    \n",
        "    # set labels and title\n",
        "    ax.set_xlabel('PC1')\n",
        "    ax.set_ylabel('PC2')\n",
        "    ax.set_title('k = ' + str(k))\n",
        "\n",
        "  # set legend for whole figure at once\n",
        "  handles, labels = fig.axes[0].get_legend_handles_labels()\n",
        "  fig.legend(handles, labels, loc='upper left', prop={'size': 20})\n",
        "\n",
        "  # set figure title\n",
        "  fig.suptitle('PC1 vs PC2 with KMeans clusters for k in [1, ' \n",
        "               + str(max_clusters) + ']', fontsize=16)\n",
        "\n",
        "  # show plot\n",
        "  plt.show()\n",
        "\n",
        "P3()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v4dXc_VloZHI"
      },
      "source": [
        "### Part 4:\n",
        "\n",
        "Fit a Gaussian Mixture Model for the positive examples in your 2d projected data. Plot the estimated density contours as shown here: http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#example-mixture-plot-gmm-pdf-py. Vary the number of mixture components from 1-4 and the covariance matrix type ('spherical', 'diag', 'tied', 'full')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZJSPb5gioZHI",
        "colab": {}
      },
      "source": [
        "def P4():\n",
        "  #initialize GMM args arrays to loop through\n",
        "  components_array = range(1,5)\n",
        "  covariancetype_array = ['spherical', 'diag', 'tied', 'full']\n",
        "\n",
        "  # fit pca and apply dimensionality reduction on X\n",
        "  pca = PCA(n_components=2)\n",
        "  reduced_train_data = pca.fit_transform(train_data)\n",
        "\n",
        "  # convert train labels to bool for use in array indexing\n",
        "  label_bools = np.array(train_labels, dtype=bool)\n",
        "\n",
        "  pos_train_data = reduced_train_data[label_bools]\n",
        "\n",
        "  for cov_type in covariancetype_array:\n",
        "    # initialize plt figure object with correct size given max_clusters\n",
        "    plot_size = 5\n",
        "    num_plots_horiz = len(components_array)\n",
        "    num_plots_vert = 1\n",
        "    fig = plt.figure(figsize=(plot_size * num_plots_horiz, \n",
        "                              plot_size * num_plots_vert))\n",
        "    fig.suptitle('GMM with \"' + cov_type + '\" covariance matrix  for positive' +\n",
        "                 ' samples with n_components in [1, ' \n",
        "                 + str(len(components_array)) + ']', fontsize=16)\n",
        "    for n_comp in components_array:\n",
        "      # add subplot\n",
        "      ax = fig.add_subplot(num_plots_vert, num_plots_horiz, n_comp)\n",
        "      ax.set_title('n = ' + str(n_comp))\n",
        "\n",
        "      # fit model\n",
        "      gmm = GaussianMixture(n_components=n_comp, covariance_type=cov_type)\n",
        "      gmm.fit(pos_train_data)\n",
        "\n",
        "      # get scores for mesh grid covering the domain\n",
        "      # set domain limits\n",
        "      x = np.linspace(-3., 3.)\n",
        "      y = np.linspace(-3., 3.)\n",
        "      # produce matrix of points\n",
        "      Xm, Ym = np.meshgrid(x, y)\n",
        "      # make array of X-vector, Y-Vector, take transpose to produce (x,y) pairs\n",
        "      XY = np.array([Xm.ravel(), Ym.ravel()]).T\n",
        "      # score points and reshape output for plotting\n",
        "      Z = -gmm.score_samples(XY)\n",
        "      Z = Z.reshape(Xm.shape)\n",
        "\n",
        "      # plot contours\n",
        "      contours = ax.contour(Xm, Ym, Z, norm=LogNorm(vmin=1.0, vmax=100.0),\n",
        "                             levels=np.logspace(0, 2, 10))\n",
        "\n",
        "      # plot points\n",
        "      ax.plot(pos_train_data[:, 0], \n",
        "              pos_train_data[:, 1], \n",
        "              'b+', label='poisonous', markersize=5)\n",
        "    \n",
        "    # add colorbar for the last plot in each figure\n",
        "    fig.colorbar(contours, shrink=0.8, extend='both')\n",
        "\n",
        "\n",
        "P4()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iMcLxdqDoZHL"
      },
      "source": [
        "### Part 5:\n",
        "\n",
        "Fit two 4-component full covariance GMMs, one for the positive examples and one for the negative examples in your 2d projected data. Predict the test examples by choosing the label for which the model gives a larger probability (use GMM.score). What is the accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AOKXtGBzoZHL",
        "colab": {}
      },
      "source": [
        "def P5():\n",
        "  cov_type='full'\n",
        "  n_comp=4\n",
        "\n",
        "  # fit pca and apply dimensionality reduction on X\n",
        "  pca = PCA(n_components=2)\n",
        "  reduced_train_data = pca.fit_transform(train_data)\n",
        "\n",
        "  # convert train labels to bool for use in array indexing\n",
        "  label_bools = np.array(train_labels, dtype=bool)\n",
        "\n",
        "  pos_train_data = reduced_train_data[label_bools]\n",
        "  neg_train_data = reduced_train_data[~label_bools]\n",
        "\n",
        "  # fit models\n",
        "  gmm_pos = GaussianMixture(n_components=n_comp, covariance_type=cov_type)\n",
        "  gmm_pos.fit(pos_train_data)\n",
        "  gmm_neg = GaussianMixture(n_components=n_comp, covariance_type=cov_type)\n",
        "  gmm_neg.fit(neg_train_data)\n",
        "\n",
        "  # get scores for each sample and model\n",
        "  pos_scores = gmm_pos.score_samples(reduced_train_data)\n",
        "  neg_scores = gmm_neg.score_samples(reduced_train_data)\n",
        "\n",
        "  # predict labels\n",
        "  train_labels_pred = [1 if score[0] >= score[1] else 0 for score \n",
        "                       in zip(pos_scores, neg_scores)]\n",
        "\n",
        "  # calculate accuracy\n",
        "  accuracy = metrics.accuracy_score(train_labels, train_labels_pred)\n",
        "  print('The accuracy is ' + str(accuracy))\n",
        "\n",
        "P5()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oWV5Ld70oZHN"
      },
      "source": [
        "### Part 6:\n",
        "\n",
        "Ideally, we'd like a model that gives the best accuracy with the fewest parameters. Run a series of experiments to find the model that gives the best accuracy with no more than 50 parameters. For example, with 3 PCA components and 2-component diagonal covariance GMMs, you'd have:\n",
        "\n",
        "( (3 mean vector + 3 covariance matrix) x 2 components ) x 2 classes = 24 parameters\n",
        "\n",
        "You should vary the number of PCA components, the number of GMM components, and the covariance type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wWCMLsM2oZHN",
        "colab": {}
      },
      "source": [
        "def get_num_params(n_pca, n_gmm, gmm_type, n_classes):\n",
        "  # each gaussian's mean is described by an n_pca-dimensional vector\n",
        "  mean_vector_comps = n_pca * n_gmm\n",
        "\n",
        "  # depending on covariance matrix type degrees of freedom are different\n",
        "  if gmm_type == 'full':\n",
        "    # number dof of n_gmm n_pca-dimensional symmetric matrices\n",
        "    n_covariance_dof = n_gmm * (n_pca * (n_pca + 1) / 2)\n",
        "  elif gmm_type == 'diag':\n",
        "    # each matrix is diagonal so only n_pca degrees of freedom per matrix\n",
        "    n_covariance_dof = n_gmm * n_pca\n",
        "  elif gmm_type == 'spherical':\n",
        "    # each component only has a single variance parameter\n",
        "    n_covariance_dof = n_gmm\n",
        "  elif gmm_type == 'tied':\n",
        "    # components share a covariance matrix\n",
        "    n_covariance_dof = n_pca * (n_pca + 1) / 2\n",
        "  else:\n",
        "    raise ValueError(\"gmm_type must take a value in \" + \n",
        "                     \"['full', 'diag', 'spherical', 'tied']\")\n",
        "\n",
        "  # multiply by n_classes because separate models for each class\n",
        "  return int(n_classes * (mean_vector_comps + n_covariance_dof))\n",
        "  \n",
        "class PcaGmmModel:\n",
        "  def __init__(self, n_pca, n_gmm, gmm_type, n_classes, accuracy):\n",
        "    self.n_pca = n_pca\n",
        "    self.n_gmm = n_gmm\n",
        "    self.gmm_type = gmm_type\n",
        "    self.accuracy = accuracy\n",
        "    self.num_params = get_num_params(n_pca, n_gmm, gmm_type, n_classes)\n",
        "\n",
        "  def __str__(self):\n",
        "    return ('Accuracy = {:.4f}'.format(self.accuracy) + ', num_params = ' \n",
        "            + str(self.num_params) + ', n_pca = ' + str(self.n_pca)\n",
        "            + ', n_gmm = ' + str(self.n_gmm) + ', gmm_type = ' + self.gmm_type)\n",
        "\n",
        "def P6():\n",
        "  max_params = 50\n",
        "\n",
        "  pca_comps = [2, 3, 4]\n",
        "  gmm_comps = [2, 3, 4]\n",
        "  gmm_types = ['spherical', 'diag', 'tied', 'full']\n",
        "\n",
        "  # convert train labels to bool for use in array indexing\n",
        "  label_bools = np.array(train_labels, dtype=bool)\n",
        "\n",
        "  # initialize vector to hold output\n",
        "  models = []\n",
        "\n",
        "  # loop through hyper-params\n",
        "  for pca_comp in pca_comps:\n",
        "    # instantiate PCA and extract reduced positive/negative data sets\n",
        "    pca = PCA(n_components=pca_comp)\n",
        "    reduced_train_data = pca.fit_transform(train_data)\n",
        "    pos_train_data = reduced_train_data[label_bools]\n",
        "    neg_train_data = reduced_train_data[~label_bools]\n",
        "    for gmm_comp in gmm_comps:\n",
        "      for gmm_type in gmm_types:\n",
        "        # check if we have too many params and skip if we do\n",
        "        num_params = get_num_params(pca_comp, gmm_comp, gmm_type, 2)\n",
        "        if num_params > max_params:\n",
        "          print('pca_comp = ' + str(pca_comp) + ', gmm_comp = ' + str(gmm_comp)\n",
        "                + \", gmm_type = '\" + gmm_type + \"' produced \" + str(num_params)\n",
        "                + ' parameters which is greater than max_params value of ' + \n",
        "                str(max_params))\n",
        "          continue\n",
        "\n",
        "        # create and fit gmm models\n",
        "        gmm_pos = GaussianMixture(n_components=gmm_comp, \n",
        "                                  covariance_type=gmm_type)\n",
        "        gmm_pos.fit(pos_train_data)\n",
        "        gmm_neg = GaussianMixture(n_components=gmm_comp, \n",
        "                                  covariance_type=gmm_type)\n",
        "        gmm_neg.fit(neg_train_data)\n",
        "\n",
        "        # get scores and predict\n",
        "        pos_scores = gmm_pos.score_samples(reduced_train_data)\n",
        "        neg_scores = gmm_neg.score_samples(reduced_train_data)\n",
        "        train_labels_pred = [1 if score[0] >= score[1] else 0 for score \n",
        "                             in zip(pos_scores, neg_scores)]\n",
        "\n",
        "        # calculate accuracy\n",
        "        accuracy = metrics.accuracy_score(train_labels, train_labels_pred)\n",
        "\n",
        "        # add to models\n",
        "        models.append(PcaGmmModel(pca_comp, gmm_comp, gmm_type, 2, accuracy))\n",
        "\n",
        "  # print output\n",
        "  models.sort(key=lambda m: m.accuracy, reverse=True)\n",
        "  print('\\nBest model considered:')\n",
        "  print(models[0])\n",
        "  print('\\nAll models considered:')\n",
        "  for model in models:\n",
        "    print(model)\n",
        "\n",
        "\n",
        "\n",
        "P6()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ZVIt7KDp28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}